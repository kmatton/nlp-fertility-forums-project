{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_seq_class_updated_comments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er4O-aYwTkE1"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YzomT7sTPPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faec5102-0142-4e76-8ae5-2822f4461c32"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeQV-5yKypGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686c4396-cbc9-4bf2-ac09-1f4ba0e60862"
      },
      "source": [
        "%%bash\n",
        "\n",
        "mkdir \"/content/gdrive/MyDrive/6864_project/\"\n",
        "mkdir \"/content/gdrive/MyDrive/6864_project/model_comment\"\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/6864_project/’: File exists\n",
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/6864_project/model_comment’: File exists\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR2s9fDgzLNs"
      },
      "source": [
        "#destination_folder_processing = \"/content/gdrive/MyDrive/6864_project/train_data/max_mean_topic\"\n",
        "\n",
        "first_n_words = 512"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSbCqzcDWn-4"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-muOECSWrOt"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNfpHFEPUcyE"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOxjGn9aki-c"
      },
      "source": [
        "def trim_string(x):\n",
        "\n",
        "    x = x.split(maxsplit=first_n_words)\n",
        "    x = ' '.join(x[:first_n_words])\n",
        "\n",
        "    return x"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0lKtYANyT3d"
      },
      "source": [
        "data = pd.read_csv(\"gdrive/MyDrive/6864_project/data/infertility/processed_posts.csv\")\n",
        "data_ttc = pd.read_csv(\"gdrive/MyDrive/6864_project/data/ttcafterloss/processed_posts.csv\")\n",
        "dfs = [data, data_ttc]\n",
        "data = pd.concat(dfs)\n",
        "topic = pd.read_csv(\"gdrive/MyDrive/6864_project/data/10_all_posts_with_comment_topics.csv\")\n",
        "data = data.merge(topic[[\"id\",\"max_mean_topic\",\"mode_max_topic\"]], on='id')\n",
        "data = data[~data.max_mean_topic.isna()]\n",
        "\n",
        "def get_bin(x):\n",
        "  if x < 4:\n",
        "    return 0\n",
        "  if x < 8:\n",
        "    return 1\n",
        "  if x < 11:\n",
        "    return 2\n",
        "  if x < 16:\n",
        "    return 3\n",
        "  if x < 25:\n",
        "    return 4\n",
        "  return 5\n",
        "\n",
        "def get_bin_upvote(x):\n",
        "  if x < .86:\n",
        "    return 0\n",
        "  if x < 1:\n",
        "    return 1\n",
        "  return 2\n",
        "\n",
        "data[\"comments_bin\"] = data[\"num_comments\"].apply(lambda x: get_bin(x))\n",
        "data[\"upvotes_bin\"] = data[\"upvote_ratio\"].apply(lambda x: get_bin_upvote(x))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "lLQwuS1YzTBv",
        "outputId": "2585b5c0-6180-41a0-c590-910274882a23"
      },
      "source": [
        "#Handling long data inputs\n",
        "\n",
        "#split into chunks of 200 words with 50 words overlapped\n",
        "def get_split(text1):\n",
        "  \n",
        "  text1 = \"\".join(text1.split(\"]\"))\n",
        "  text1 = \"\".join(text1.split(\"[\"))\n",
        "\n",
        "  text1 = \" \".join(text1.split(\",\"))\n",
        "\n",
        "  text1 = text1.replace(\"'\", \"\")\n",
        "\n",
        "  return text1[:first_n_words]\n",
        "\n",
        "data = data[data.processed_text.str.len() > 10]\n",
        "data[\"processed_text\"] = data[\"processed_title\"] + \",\" + data[\"processed_text\"]\n",
        "data[\"text_split\"] = data[\"processed_text\"].apply(get_split)\n",
        "data.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>author</th>\n",
              "      <th>author_fullname</th>\n",
              "      <th>author_flair_text</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>selftext</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>score</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>data_split</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>processed_title</th>\n",
              "      <th>max_mean_topic</th>\n",
              "      <th>mode_max_topic</th>\n",
              "      <th>comments_bin</th>\n",
              "      <th>upvotes_bin</th>\n",
              "      <th>text_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2t72mf</td>\n",
              "      <td>1.421865e+09</td>\n",
              "      <td>nhmejia</td>\n",
              "      <td>t2_ivw2a</td>\n",
              "      <td>Hard to get pregnant after a hysto.</td>\n",
              "      <td>https://www.reddit.com/r/infertility/comments/...</td>\n",
              "      <td>*TRIGGER WARNING* Not that funny...</td>\n",
              "      <td>I've been doing really well staying away from ...</td>\n",
              "      <td>0.80</td>\n",
              "      <td>8</td>\n",
              "      <td>25</td>\n",
              "      <td>val</td>\n",
              "      <td>[['trigger', 'warn', 'not', 'that', 'funny']],...</td>\n",
              "      <td>[['trigger', 'warn', 'not', 'that', 'funny']]</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>trigger  warn  not  that  funny I  be  do  rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2t6u8l</td>\n",
              "      <td>1.421862e+09</td>\n",
              "      <td>septicidal</td>\n",
              "      <td>t2_6h9ib</td>\n",
              "      <td>PCOS</td>\n",
              "      <td>https://www.reddit.com/r/infertility/comments/...</td>\n",
              "      <td>Update on planning for a new cycle</td>\n",
              "      <td>I know a bunch of people saw my update in the ...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>train</td>\n",
              "      <td>[['update', 'on', 'plan', 'for', 'a', 'new', '...</td>\n",
              "      <td>[['update', 'on', 'plan', 'for', 'a', 'new', '...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>update  on  plan  for  a  new  cycle I  know  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2t57np</td>\n",
              "      <td>1.421820e+09</td>\n",
              "      <td>PoisonRose23</td>\n",
              "      <td>t2_j2zbu</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.reddit.com/r/infertility/comments/...</td>\n",
              "      <td>Decision about fermara! I need advice!!!</td>\n",
              "      <td>Long post warning: So I have a decision to mak...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>train</td>\n",
              "      <td>[['decision', 'about', 'fermara', 'I', 'need',...</td>\n",
              "      <td>[['decision', 'about', 'fermara', 'I', 'need',...</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>decision  about  fermara  I  need  advice long...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>2t45rt</td>\n",
              "      <td>1.421801e+09</td>\n",
              "      <td>Pamzella</td>\n",
              "      <td>t2_aifrd</td>\n",
              "      <td>41 MFI &amp; DOR 1MC mult IUIs</td>\n",
              "      <td>https://www.reddit.com/r/infertility/comments/...</td>\n",
              "      <td>\"Shared risk\" clinics</td>\n",
              "      <td>Please educate me, someone asked me about this...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>7</td>\n",
              "      <td>17</td>\n",
              "      <td>train</td>\n",
              "      <td>[['share', 'risk', 'clinic']],[['please', 'edu...</td>\n",
              "      <td>[['share', 'risk', 'clinic']]</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>share  risk  clinic please  educate  I  someon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>2t3x6r</td>\n",
              "      <td>1.421797e+09</td>\n",
              "      <td>fumbellina</td>\n",
              "      <td>t2_iibv6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.reddit.com/r/infertility/comments/...</td>\n",
              "      <td>FET anxiety</td>\n",
              "      <td>My FET is scheduled for next week, and I'm fee...</td>\n",
              "      <td>0.81</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>train</td>\n",
              "      <td>[['fet', 'anxiety']],[['my', 'fet', 'be', 'sch...</td>\n",
              "      <td>[['fet', 'anxiety']]</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>fet  anxiety my  fet  be  schedule  for  next ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         text_split\n",
              "0           0  ...  trigger  warn  not  that  funny I  be  do  rea...\n",
              "1           1  ...  update  on  plan  for  a  new  cycle I  know  ...\n",
              "2           3  ...  decision  about  fermara  I  need  advice long...\n",
              "3           5  ...  share  risk  clinic please  educate  I  someon...\n",
              "4           6  ...  fet  anxiety my  fet  be  schedule  for  next ...\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl4_dITCUk83"
      },
      "source": [
        "# # Read raw data\n",
        "# df_raw = data\n",
        "\n",
        "# # Prepare columns\n",
        "# df_raw['label'] = df_raw[\"max_mean_topic\"]\n",
        "# df_raw['titletext'] = df_raw[\"text_split\"]\n",
        "# df_raw = df_raw.reindex(columns=['label','titletext',\"id\",\"data_split\"])\n",
        "\n",
        "# # Drop rows with empty text\n",
        "# df_raw.drop( df_raw[df_raw.titletext.str.len() < 5].index, inplace=True)\n",
        "\n",
        "# # Trim text and titletext to first_n_words\n",
        "# # df_raw['text'] = df_raw['text'].apply(trim_string)\n",
        "# df_raw['titletext'] = df_raw['titletext'].apply(trim_string) \n",
        "\n",
        "# df_train = df_raw[df_raw.data_split == \"train\"]\n",
        "# df_train = df_raw[df_raw.data_split == \"val\"]\n",
        "# df_train = df_raw[df_raw.data_split == \"test\"]\n",
        "\n",
        "# # # Split according to label\n",
        "# # df_real = df_raw[df_raw['label'] == 0]\n",
        "# # df_fake = df_raw[df_raw['label'] == 1]\n",
        "\n",
        "# # Train-test split\n",
        "# #df_train, df_test = train_test_split(df_raw, train_size = train_test_ratio, random_state = 1)\n",
        "# # df_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = train_test_ratio, random_state = 1)\n",
        "\n",
        "# # Train-valid split\n",
        "# #df_train, df_valid = train_test_split(df_train, train_size = train_valid_ratio, random_state = 1)\n",
        "# # df_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state = 1)\n",
        "\n",
        "# # Concatenate splits of different labels\n",
        "# # df_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\n",
        "# # df_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\n",
        "# # df_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)\n",
        "\n",
        "# # Write preprocessed data\n",
        "# df_train.to_csv(destination_folder_processing + '/train.csv', index=False)\n",
        "# df_valid.to_csv(destination_folder_processing + '/valid.csv', index=False)\n",
        "# df_test.to_csv(destination_folder_processing + '/test.csv', index=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPt_6srt1i7Y",
        "outputId": "d9a2c1c8-82b7-41e2-ced9-5e71c9d8b5c7"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgZzb6JX6FHd"
      },
      "source": [
        "# !pip install torch==1.6 torchtext==0.7"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5L5wJDO1xwa"
      },
      "source": [
        "# !pip install -U torchtext==0.9.0"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_wj8UnU2Bbx"
      },
      "source": [
        "# !pip install -U torch==1.8.1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7H0eVyh4l9-",
        "outputId": "0190be3d-0427-4b10-e663-2a2a58b992be"
      },
      "source": [
        "!pip list | grep torch"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch                         1.8.1+cu101   \n",
            "torchsummary                  1.5.1         \n",
            "torchtext                     0.9.1         \n",
            "torchvision                   0.9.1+cu101   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_5vd791kDI"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "# from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "import transformers\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2r36Lr81m_h",
        "outputId": "09da11ee-11e0-4c6f-cba9-7c0cc8510897"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "#device=\"cpu\"\n",
        "print(device)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2KmLcZm2cbZ"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpYcGK9y_2qe"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df):\n",
        "        'Initialization'\n",
        "        self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "        # Load data and get label\n",
        "        row = self.df.iloc[index]\n",
        "        encode = tokenizer.encode_plus(row.text_split, add_special_tokens=True, max_length=512, padding='max_length', return_attention_mask=True,return_tensors='pt')\n",
        "        input_ids = torch.cat([x.float() for x in encode[\"input_ids\"]], dim=0).to(torch.int64)\n",
        "        attention_mask = torch.cat([x.float() for x in encode[\"attention_mask\"]], dim=0)\n",
        "        #label = torch.tensor(row.num_comments).to(torch.float)\n",
        "        label = torch.tensor(row.comments_bin).to(torch.long)\n",
        "        #label = torch.tensor(row.upvote_ratio).to(torch.float)\n",
        "        #label = torch.tensor(row.mode_max_topic).to(torch.long)\n",
        "        id = row.id\n",
        "        sample = {\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"label\":label, \"id\":id}\n",
        "        #print(sample[\"label\"], row.num_comments)\n",
        "        return sample\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pthryzw7A4Km"
      },
      "source": [
        "BATCH_SIZE= 16\n",
        "def custom_collate(batch):\n",
        "  return {\"input_ids\":nn.utils.rnn.pad_sequence(stack_vals(batch,\"input_ids\"),batch_first=True), \"attention_mask\":nn.utils.rnn.pad_sequence(stack_vals(batch,\"attention_mask\"),batch_first=True), \"label\":torch.tensor(stack_vals(batch,\"label\"))}\n",
        "\n",
        "train_df = data[data.data_split==\"train\"]\n",
        "train_ds = Dataset(train_df)\n",
        "#train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,collate_fn=custom_collate)\n",
        "train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_df = data[data.data_split==\"val\"]\n",
        "val_ds = Dataset(val_df)\n",
        "#val_ldr = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True,collate_fn=custom_collate)\n",
        "val_ldr = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_df = data[data.data_split==\"test\"]\n",
        "test_ds = Dataset(test_df)\n",
        "#val_ldr = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True,collate_fn=custom_collate)\n",
        "test_ldr = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOSJoIoe2egC"
      },
      "source": [
        "# # Model parameter\n",
        "# MAX_SEQ_LEN = 128\n",
        "# PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "# UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "\n",
        "# # Fields\n",
        "\n",
        "# label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "# text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "#                    fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "# fields = [('label', label_field),('titletext', text_field)]\n",
        "\n",
        "# # TabularDataset\n",
        "\n",
        "# train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
        "#                                            test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "\n",
        "# # Iterators\n",
        "\n",
        "# train_iter = BucketIterator(train, batch_size=8, sort_key=lambda x: len(x.titletext),\n",
        "#                             device=device, train=True, sort=True, sort_within_batch=True)\n",
        "# valid_iter = BucketIterator(valid, batch_size=8, sort_key=lambda x: len(x.titletext),\n",
        "#                             device=device, train=True, sort=True, sort_within_batch=True)\n",
        "# test_iter = Iterator(test, batch_size=8, device=device, train=False, shuffle=False, sort=False)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZdqxTJ52euH"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BERT, self).__init__()\n",
        "        self.encoder = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", config = config)\n",
        "\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea,hidden = self.encoder(input_ids=text, labels=label,output_hidden_states=True)[:3]\n",
        "        #print(text_fea.shape)\n",
        "\n",
        "        return loss, text_fea, hidden[-1]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ6o3WE27ATb"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCqGw31k7IQy"
      },
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.CrossEntropyLoss(),\n",
        "          train_loader = train_ldr,\n",
        "          valid_loader = val_ldr,\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_ldr) // 2,\n",
        "          file_path = destination_folder,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for index, batch in enumerate(train_loader):\n",
        "            labels = batch[\"label\"].type(torch.LongTensor)           \n",
        "            labels = labels.to(device)\n",
        "            titletext = batch[\"input_ids\"].type(torch.LongTensor)  \n",
        "            titletext = titletext.to(device)\n",
        "            mask = batch[\"attention_mask\"].type(torch.LongTensor)  \n",
        "            mask = titletext.to(device)\n",
        "\n",
        "            output = model(titletext, label=labels)\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            if index % 20 == 1:\n",
        "              print(f'At step {index *16}, the loss = {loss}, {running_loss / index}')\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for batch in valid_loader:\n",
        "                        labels = batch[\"label\"].type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        titletext = batch[\"input_ids\"].type(torch.LongTensor)  \n",
        "                        titletext = titletext.to(device)\n",
        "                        mask = batch[\"attention_mask\"].type(torch.LongTensor)  \n",
        "                        mask = titletext.to(device)\n",
        "\n",
        "                        output = model(titletext, label=labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR8wxqYj7Qsh"
      },
      "source": [
        "\n",
        "destination_folder = \"/content/gdrive/MyDrive/6864_project/model_comment\"\n",
        "\n",
        "from transformers import DistilBertConfig\n",
        "config = DistilBertConfig.from_pretrained('distilbert-base-uncased',num_labels=6)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkFbeKItXpnU"
      },
      "source": [
        "#DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", config = config)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0sXRZIp7MM7",
        "outputId": "0595529c-d4d3-4432-e83e-43d416701ba1"
      },
      "source": [
        "import os\n",
        "print(device)\n",
        "#device= \"cpu\"\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "model = BERT(config).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "#train(model=model, optimizer=optimizer)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9SOXuAREfRc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "0960e307-6e2b-4fea-d879-dc0ad0f46c4b"
      },
      "source": [
        "load_checkpoint(destination_folder + '/' + 'model.pt', model)\n",
        "from sklearn.metrics import f1_score,  accuracy_score\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_true = []\n",
        "    y_pred = []   \n",
        "    valid_running_loss = 0               \n",
        "\n",
        "    # validation loop\n",
        "    for batch in test_ldr:\n",
        "        labels = batch[\"label\"].type(torch.LongTensor)           \n",
        "        labels = labels.to(device)\n",
        "        titletext = batch[\"input_ids\"].type(torch.LongTensor)  \n",
        "        titletext = titletext.to(device)\n",
        "\n",
        "        output = model(titletext, label=labels)\n",
        "        loss, logits = output\n",
        "        bin = torch.argmax(logits,dim=1)\n",
        "        #print(batch[\"label\"], bin, bin.shape)\n",
        "        y_true.append(batch[\"label\"])\n",
        "        y_pred.append(bin)\n",
        "      \n",
        "        valid_running_loss += loss.item()\n",
        "\n",
        "# evaluation\n",
        "\n",
        "from sklearn.metrics import f1_score,  accuracy_score\n",
        "import numpy as np\n",
        "print(valid_running_loss / len(test_ldr))\n",
        "y_true = torch.cat(y_true).detach().cpu().numpy().flatten()\n",
        "y_pred = torch.cat(y_pred).detach().cpu().numpy().flatten()\n",
        "print(y_true, y_pred)\n",
        "print(\"accuracy\", accuracy_score(y_true, y_pred, average=\"micro\"))\n",
        "print(\"f1\", f1_score(y_true, y_pred, average=\"micro\"))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded from <== /content/gdrive/MyDrive/6864_project/model_comment/model.pt\n",
            "1.649937872974961\n",
            "[1 2 2 ... 1 1 1] [1 1 1 ... 1 4 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-344bcb878261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: accuracy_score() got an unexpected keyword argument 'average'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "P4h29jZdoK69",
        "outputId": "5b38a04c-45aa-4f34-ac2b-4b08bedbf96b"
      },
      "source": [
        "from sklearn.metrics import f1_score,  accuracy_score\n",
        "import numpy as np\n",
        "print(valid_running_loss / len(test_ldr))\n",
        "y_true = torch.cat(y_true).detach().cpu().numpy().flatten()\n",
        "y_pred = torch.cat(y_pred).detach().cpu().numpy().flatten()\n",
        "print(y_true, y_pred)\n",
        "print(\"accuracy\", accuracy_score(y_true, y_pred,average=\"micro\"))\n",
        "print(\"f1\", f1_score(y_true, y_pred,average=\"micro\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.699371353343681\n",
            "[1 4 3 ... 3 3 3] [1 3 1 ... 0 4 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0670e3f39ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"micro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: accuracy_score() got an unexpected keyword argument 'average'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9L5YqnjwBwn"
      },
      "source": [
        "np.savetxt(\"/content/gdrive/MyDrive/6864_project/pred_comments.txt\", np.array([y_true,y_pred]))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "id": "-KTKv_C4rgUG",
        "outputId": "f31ea606-19a1-412d-af00-2327b3e3de71"
      },
      "source": [
        "load_checkpoint(destination_folder + '/' + 'model.pt', model)\n",
        "model.eval()\n",
        "outputs= []\n",
        "for index, batch in enumerate(train_ldr):\n",
        "    labels = batch[\"label\"].type(torch.LongTensor)           \n",
        "    labels = labels.to(device)\n",
        "    titletext = batch[\"input_ids\"].type(torch.LongTensor)  \n",
        "    titletext = titletext.to(device)\n",
        "\n",
        "    output = model(titletext, label=labels)\n",
        "    loss, logits, hidden = output\n",
        "    embed = hidden[:,0,:]\n",
        "    if index % 20 == 1:\n",
        "      print(index*16)\n",
        "    #print(embed.shape)\n",
        "    for i in range(16):\n",
        "      latest_output = {\"id\": batch[\"id\"][i],\"label\": batch[\"label\"].numpy()[i], \"prediction\":torch.argmax(logits,dim=1).detach().cpu().numpy()[i],\"embedding\":embed.detach().cpu().numpy()[i] }\n",
        "      #print(embed.detach().cpu().numpy()[i].shape)\n",
        "      outputs.append(latest_output)\n",
        "    #print(latest_output)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded from <== /content/gdrive/MyDrive/6864_project/model_comment/model.pt\n",
            "16\n",
            "336\n",
            "656\n",
            "976\n",
            "1296\n",
            "1616\n",
            "1936\n",
            "2256\n",
            "2576\n",
            "2896\n",
            "3216\n",
            "3536\n",
            "3856\n",
            "4176\n",
            "4496\n",
            "4816\n",
            "5136\n",
            "5456\n",
            "5776\n",
            "6096\n",
            "6416\n",
            "6736\n",
            "7056\n",
            "7376\n",
            "7696\n",
            "8016\n",
            "8336\n",
            "8656\n",
            "8976\n",
            "9296\n",
            "9616\n",
            "9936\n",
            "10256\n",
            "10576\n",
            "10896\n",
            "11216\n",
            "11536\n",
            "11856\n",
            "12176\n",
            "12496\n",
            "12816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-936fd9e766e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#print(embed.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mlatest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0;31m#print(embed.detach().cpu().numpy()[i].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzG90RZh5_Yw"
      },
      "source": [
        "df = pd.DataFrame(outputs)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbpEmaJA6A06"
      },
      "source": [
        "df.to_csv(\"/content/gdrive/MyDrive/6864_project/bert_seq_class_comments.csv\")"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CG3nY9Wm6Eyu",
        "outputId": "a4258112-bebc-4eec-ca1d-5cdf653fa703"
      },
      "source": [
        "df"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>prediction</th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d5k1p9</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.17213681, 0.12055801, 0.008012729, -0.1584...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fiaoe6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.32413787, -0.09687651, 0.049001757, -0.380...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3k4gvy</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[-0.1798693, -0.10688318, -0.08189541, -1.0396...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8orpvf</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.026685385, -0.12186421, 0.05015227, 0.04255...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>e9wo1p</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>[-0.27250776, -0.56289303, 0.1940978, 0.183885...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12889</th>\n",
              "      <td>ekx90i</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.47269633, -0.516186, 0.20204246, -0.143793...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12890</th>\n",
              "      <td>88w2e9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.54807967, -0.3665095, 0.059043285, -0.1831...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12891</th>\n",
              "      <td>avdhev</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.4859991, -0.2646182, 0.3061617, -0.0750821...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12892</th>\n",
              "      <td>f0x78o</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.3363064, -0.118753195, 0.27506822, -0.4199...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12893</th>\n",
              "      <td>gwr904</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.15161538, -0.52141756, -0.03593611, -0.3601...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12894 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ...                                          embedding\n",
              "0      d5k1p9  ...  [-0.17213681, 0.12055801, 0.008012729, -0.1584...\n",
              "1      fiaoe6  ...  [-0.32413787, -0.09687651, 0.049001757, -0.380...\n",
              "2      3k4gvy  ...  [-0.1798693, -0.10688318, -0.08189541, -1.0396...\n",
              "3      8orpvf  ...  [0.026685385, -0.12186421, 0.05015227, 0.04255...\n",
              "4      e9wo1p  ...  [-0.27250776, -0.56289303, 0.1940978, 0.183885...\n",
              "...       ...  ...                                                ...\n",
              "12889  ekx90i  ...  [-0.47269633, -0.516186, 0.20204246, -0.143793...\n",
              "12890  88w2e9  ...  [-0.54807967, -0.3665095, 0.059043285, -0.1831...\n",
              "12891  avdhev  ...  [-0.4859991, -0.2646182, 0.3061617, -0.0750821...\n",
              "12892  f0x78o  ...  [-0.3363064, -0.118753195, 0.27506822, -0.4199...\n",
              "12893  gwr904  ...  [0.15161538, -0.52141756, -0.03593611, -0.3601...\n",
              "\n",
              "[12894 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XdsHFpn7UAE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}